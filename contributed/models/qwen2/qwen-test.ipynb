{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, GenerationConfig\n",
    "from modeling_qwen import Qwen2InferenceConfig, NeuronQwen2ForCausalLM\n",
    "from neuronx_distributed_inference.models.config import NeuronConfig, OnDeviceSamplingConfig\n",
    "from neuronx_distributed_inference.utils.hf_adapter import HuggingFaceGenerationAdapter, load_pretrained_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/ubuntu/model_hf_qwq/qwq/\"\n",
    "traced_model_path = \"/home/ubuntu/traced_model_qwq/qwq/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "HfFolder.save_token(\"YOUR TOKEN HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\"Qwen/QwQ-32B\", local_dir=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_qwen import Qwen2InferenceConfig, NeuronQwen2ForCausalLM\n",
    "\n",
    "def run_qwq_compile():\n",
    "    # Initialize configs and tokenizer.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "    generation_config_kwargs = {\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 1,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    }\n",
    "    generation_config.update(**generation_config_kwargs)\n",
    " \n",
    "    neuron_config = NeuronConfig(\n",
    "        tp_degree=8,\n",
    "        batch_size=1,\n",
    "        max_context_length=4096,\n",
    "        seq_len=8096,\n",
    "        on_device_sampling_config=OnDeviceSamplingConfig(top_k=5),\n",
    "        enable_bucketing=True,\n",
    "        context_encoding_buckets=[128, 1024, 4096],\n",
    "        token_generation_buckets=[128, 1024, 8096],\n",
    "        flash_decoding_enabled=False,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        fused_qkv=False,\n",
    "        attn_cls=\"NeuronQwen2Attention\"\n",
    "    )\n",
    "    config = Qwen2InferenceConfig(\n",
    "        neuron_config,\n",
    "        load_config=load_pretrained_config(model_path),\n",
    "    )\n",
    "    \n",
    "    # Compile and save model.\n",
    "    print(\"\\nCompiling and saving model...\")\n",
    "    model = NeuronQwen2ForCausalLM(model_path, config)\n",
    "    model.compile(traced_model_path)\n",
    "    tokenizer.save_pretrained(traced_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_qwq_compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuronQwen2ForCausalLM(traced_model_path)\n",
    "model.load(traced_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(traced_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "generation_config_kwargs = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 5,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "}\n",
    "generation_config.update(**generation_config_kwargs)\n",
    "generation_model = HuggingFaceGenerationAdapter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of prompts\n",
    "prompts = [\n",
    "    \"How many r's are in the word \\\"strawberry\\\"\",\n",
    "]\n",
    "\n",
    "# Create messages for each prompt\n",
    "messages_list = [\n",
    "    [{\"role\": \"user\", \"content\": prompt}] for prompt in prompts\n",
    "]\n",
    "\n",
    "# Apply chat template to each set of messages\n",
    "texts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    ) for messages in messages_list\n",
    "]\n",
    "\n",
    "# Tokenize the batch of texts\n",
    "model_inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating outputs...\")\n",
    "outputs = generation_model.generate(\n",
    "    **model_inputs,\n",
    "    generation_config=generation_config,\n",
    "    max_length=model.config.neuron_config.max_length,\n",
    ")\n",
    "output_tokens = tokenizer.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generated outputs:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Output 0: user\\nHow many r\\'s are in the word \"strawberry\"\\nassistant\\n<think>\\nOkay, so I need to figure out how many times the letter \\'r\\' appears in the word \"strawberry.\" Let me start by writing down the word and looking at each letter one by one. \\n\\nFirst, I\\'ll spell out \"strawberry\" to make sure I have all the letters right. S-T-R-A-W-B-E-R-R-Y. Wait, let me check that again. Sometimes I might miss a letter. Let me count the letters as I write them:\\n\\n1. S\\n2. T\\n3. R\\n4. A\\n5. W\\n6. B\\n7. E\\n8. R\\n9. R\\n10. Y\\n\\nHmm, so that\\'s 10 letters in total. Now, I need to count how many times \\'R\\' shows up. Let me go through each letter again and note the positions where \\'R\\' is.\\n\\nStarting from the first letter:\\n1. S – not an R\\n2. T – not an R\\n3. R – that\\'s the first R\\n4. A – no\\n5. W – no\\n6. B – no\\n7. E – no\\n8. R – second R\\n9. R – third R\\n10. Y – no\\n\\nWait a second, so after the first R at position 3, the next R is at position 8, and then another at 9? Let me confirm the spelling again because sometimes people might confuse \"strawberry\" with other similar words. Let me think: S-T-R-A-W-B-E-R-R-Y. Yes, that\\'s correct. After the \\'E\\', there are two R\\'s in a row, right? So positions 8 and 9 are both R\\'s. So that would make three R\\'s in total: one at position 3, and two at 8 and 9. \\n\\nBut hold on, maybe I miscounted the letters. Let me write them out again with numbers to be sure:\\n\\n1. S\\n2. T\\n3. R\\n4. A\\n5. W\\n6. B\\n7. E\\n8. R\\n9. R\\n10. Y\\n\\nYes, that\\'s correct. So the letters R are at positions 3, 8, and 9. That\\'s three R\\'s. Wait, but sometimes when I say \"strawberry,\" I might not pronounce the second R as clearly, but spelling-wise, it\\'s definitely there. Let me check another way. Maybe breaking the word into parts. \"Straw\" and \"berry.\" In \"straw,\" there\\'s an R. Then in \"berry,\" which is B-E-R-R-Y. So \"berry\" has two R\\'s. So adding the one from \"straw,\" that\\'s three total. \\n\\nAlternatively, maybe I can think of the word as S-T-R-A-W-B-E-R-R-Y. So breaking it down:\\n\\n- S T R (so first R)\\n- A W B E (no R\\'s here)\\n- R R Y (two more R\\'s)\\n\\nSo that\\'s 1 + 2 = 3 R\\'s. \\n\\nI think that\\'s right, but I want to be absolutely sure. Let me try another approach. Let me write the word and circle each R:\\n\\nS T R A W B E R R Y\\n\\nCircling the R\\'s: the third letter is R, then the eighth and ninth letters are R\\'s. So three in total. \\n\\nAlternatively, maybe I can use a different method. Let me count the letters one by one and tally the R\\'s:\\n\\nStarting with S: 0\\nT: 0\\nR: 1\\nA: 1\\nW:1\\nB:1\\nE:1\\nR: 2\\nR:3\\nY:3\\n\\nWait, no, that\\'s not the right way. Each time I see an R, I should increment the count. Let me try again:\\n\\n1. S – count remains 0\\n2. T – 0\\n3. R – count becomes 1\\n4. A – 1\\n5. W –1\\n6. B –1\\n7. E –1\\n8. R – count becomes 2\\n9. R – count becomes 3\\n10. Y –3\\n\\nYes, so the final count is 3. \\n\\nI think I might have confused myself earlier when I thought maybe two, but upon multiple checks, it\\'s three. Let me see if any sources or examples say otherwise. Wait, maybe I should just confirm by looking up the spelling of \"strawberry\" again. \\n\\nLooking it up in my mind: S-T-R-A-W-B-E-R-R-Y. Yes, that\\'s correct. The standard spelling has three R\\'s. So the answer should be three. \\n\\nAlternatively, maybe I can think of the pronunciation. In some accents, the double R in \"berry\" might be pronounced as a single sound, but that doesn\\'t change the spelling. The question is about the written word, so the letters are what matter, not the pronunciation. \\n\\nTherefore, after carefully going through each letter and multiple methods of counting, I can confidently say there are three R\\'s in \"strawberry.\"\\n</think>\\n\\nThe word \"strawberry\" contains three instances of the letter \\'r\\'. Here\\'s the breakdown:\\n\\n1. **S**  \\n2. **T**  \\n3. **R** (1st \\'r\\')  \\n4. **A**  \\n5. **W**  \\n6. **B**  \\n7. **E**  \\n8. **R** (2nd \\'r\\')  \\n9. **R** (3rd \\'r\\')  \\n10. **Y**  \\n\\n**Answer:** There are **3 r\\'s** in the word \"strawberry.\"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "display(\"Generated outputs:\")\n",
    "for i, output_token in enumerate(output_tokens):\n",
    "    display(f\"Output {i}: {output_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Token Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/opt/aws_neuronx_venv_pytorch_2_5_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/'\n",
    "!cp modeling_qwen.py {dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp {dir}/inference_demo.py ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add the following to the inference_demo.py we just copied to our working directory\n",
    "\n",
    "```\n",
    "from .modeling_qwen import NeuronQwen2ForCausalLM\n",
    "\n",
    "MODEL_TYPES = {\n",
    "    \"llama\": {\"causal-lm\": NeuronLlamaForCausalLM},\n",
    "    \"mixtral\": {\"causal-lm\": NeuronMixtralForCausalLM},\n",
    "    \"dbrx\": {\"causal-lm\": NeuronDbrxForCausalLM},\n",
    "    \"qwen\": {'causal-lm': NeuronQwen2ForCausalLM} #add this line\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./inference_demo.py {dir}/inference_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restart your kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!inference_demo \\\n",
    "    --model-type qwen \\\n",
    "    --task-type causal-lm \\\n",
    "    run \\\n",
    "    --model-path /home/ubuntu/model_hf_qwq/qwq/ \\\n",
    "    --compiled-model-path /home/ubuntu/traced_model_qwq/qwq/ \\\n",
    "    --torch-dtype bfloat16 \\\n",
    "    --tp-degree 8 \\\n",
    "    --batch-size 1 \\\n",
    "    --max-context-length 32 \\\n",
    "    --seq-len 64 \\\n",
    "    --on-device-sampling \\\n",
    "    --enable-bucketing \\\n",
    "    --top-k 1 \\\n",
    "    --do-sample \\\n",
    "    --pad-token-id 32000 \\\n",
    "    --prompt \"To be, or not to be\" \\\n",
    "    --check-accuracy-mode token-matching \\\n",
    "    --benchmark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_2_5_nxd_inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
