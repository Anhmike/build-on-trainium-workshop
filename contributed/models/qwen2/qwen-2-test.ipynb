{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libneuronxla                  2.2.3493.0+78c3e78c\n",
      "neuronx-cc                    2.18.121.0+9e31e41a\n",
      "neuronx-distributed           0.12.12111+cdd84048\n",
      "neuronx-distributed-inference 0.3.5591+f50feae2\n",
      "torch-neuronx                 2.6.0.2.7.5413+113e6810\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GenerationConfig\n",
    "from neuronx_distributed_inference.models.config import NeuronConfig, OnDeviceSamplingConfig\n",
    "from neuronx_distributed_inference.utils.hf_adapter import HuggingFaceGenerationAdapter, load_pretrained_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/ubuntu/model_hf_qwen/qwen2/\"\n",
    "traced_model_path = \"/home/ubuntu/traced_model_qwen/qwen2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\"Qwen/QwQ-32B\", local_dir=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_qwen_v2 import Qwen2InferenceConfig, NeuronQwen2ForCausalLM\n",
    "\n",
    "def run_qwen2_compile():\n",
    "    # Initialize configs and tokenizer.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "    generation_config_kwargs = {\n",
    "        \"do_sample\": False,\n",
    "        \"top_k\": 1,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    }\n",
    "    generation_config.update(**generation_config_kwargs)\n",
    " \n",
    "    neuron_config = NeuronConfig(\n",
    "        tp_degree=8,\n",
    "        batch_size=1,\n",
    "        max_context_length=128,\n",
    "        seq_len=256,\n",
    "        enable_bucketing=True,\n",
    "        context_encoding_buckets=[128],\n",
    "        token_generation_buckets=[256],\n",
    "        flash_decoding_enabled=False,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        fused_qkv=False,\n",
    "        attn_kernel_enabled=True,\n",
    "        attn_cls=\"NeuronQwen2Attention\"\n",
    "    )\n",
    "    config = Qwen2InferenceConfig(\n",
    "        neuron_config,\n",
    "        load_config=load_pretrained_config(model_path),\n",
    "    )\n",
    "    \n",
    "    # Compile and save model.\n",
    "    print(\"\\nCompiling and saving model...\")\n",
    "    model = NeuronQwen2ForCausalLM(model_path, config)\n",
    "    model.compile(traced_model_path)\n",
    "    tokenizer.save_pretrained(traced_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_qwen2_compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_qwen_v2 import Qwen2InferenceConfig, NeuronQwen2ForCausalLM\n",
    "\n",
    "model = NeuronQwen2ForCausalLM(traced_model_path)\n",
    "model.load(traced_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model.get_config_cls()\n",
    "config.get_neuron_config_cls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_key_value_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5120"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Okay, the user wants a short introduction to large language models. Let me start by defining what a large language model is. I should mention that they are AI systems trained on vast amounts of text data. Maybe include that they use deep learning, specifically transformer architectures.\\n\\nI need to highlight their capabilities, like generating text, understanding context, and performing various tasks such as answering questions, writing stories, or coding. It's important to note their scaleâ€”large parameter counts and extensive training data. \\n\\nAlso, touch on their applications: customer service, content creation, research, etc. Maybe mention some examples like GPT, BERT, or\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(traced_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "generation_config_kwargs = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 5,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "}\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "generation_model = HuggingFaceGenerationAdapter(model)\n",
    "generated_ids = generation_model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=128\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/ubuntu/model_hf_qwen/qwen2\"\n",
    "traced_model_path = \"/home/ubuntu/traced_model_qwen/qwen2/logit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/'\n",
    "!cp modeling_qwen2.py {dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit the inference_demo.py file to include the following:\n",
    "\n",
    "```python\n",
    "from .modeling_qwen2 import NeuronQwen2ForCausalLM\n",
    "\n",
    "MODEL_TYPES = {\n",
    "    \"llama\": {\"causal-lm\": NeuronLlamaForCausalLM},\n",
    "    \"mixtral\": {\"causal-lm\": NeuronMixtralForCausalLM},\n",
    "    \"dbrx\": {\"causal-lm\": NeuronDbrxForCausalLM},\n",
    "    'qwen2': {\"causal-lm\": NeuronQwen2ForCausalLM}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/modules/attention/utils.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.custom_calls import neuron_cumsum\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: Set seed for `privateuseone` device does not take effect, please add API's `_is_in_bad_fork` and `manual_seed_all` to `privateuseone` device module.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_model.py:12: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.gqa import GQA, GroupQueryAttention_QKV\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_model.py:12: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.gqa import GQA, GroupQueryAttention_QKV\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_model.py:12: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.gqa import GQA, GroupQueryAttention_QKV\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/models/dbrx/modeling_dbrx.py:38: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.attention_base import NeuronAttentionBase\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/models/dbrx/modeling_dbrx.py:38: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.attention_base import NeuronAttentionBase\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/inference_demo.py:25: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.models.dbrx.modeling_dbrx import NeuronDbrxForCausalLM\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/inference_demo.py:27: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.models.mixtral.modeling_mixtral import NeuronMixtralForCausalLM\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/models/mllama/modeling_mllama.py:72: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .modeling_mllama_vision import NeuronMllamaVisionModel  # noqa: E402\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/utils/accuracy.py:29: UserWarning: Intel extension for pytorch not found. For faster CPU references install `intel-extension-for-pytorch`.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: Set seed for `privateuseone` device does not take effect, please add API's `_is_in_bad_fork` and `manual_seed_all` to `privateuseone` device module.\n",
      "  return fn(*args, **kwargs)\n",
      "Loading configs...\n",
      "WARNING:root:NeuronConfig init: Unexpected keyword arguments: {'model_type': 'qwen2', 'task_type': 'causal-lm', 'model_path': '/home/ubuntu/model_hf_qwen/qwen2', 'compiled_model_path': '/home/ubuntu/traced_model_qwen/qwen2/logit', 'benchmark': True, 'check_accuracy_mode': <CheckAccuracyMode.LOGIT_MATCHING: 'logit-matching'>, 'divergence_difference_tol': 0.001, 'prompts': ['To be, or not to be'], 'top_k': 1, 'top_p': 1.0, 'temperature': 1.0, 'do_sample': False, 'dynamic': False, 'pad_token_id': 151645, 'on_device_sampling': False, 'enable_torch_dist': False, 'enable_lora': False, 'max_loras': 1, 'max_lora_rank': 16, 'skip_warmup': False, 'skip_compile': False, 'compile_only': False, 'compile_dry_run': False, 'hlo_debug': False}\n",
      "\n",
      "Compiling and saving model...\n",
      "INFO:Neuron:Generating HLOs for the following models: ['context_encoding_model', 'token_generation_model']\n",
      "[2025-06-02 13:35:56.009: I neuronx_distributed/parallel_layers/parallel_state.py:592] > initializing tensor model parallel with size 8\n",
      "[2025-06-02 13:35:56.009: I neuronx_distributed/parallel_layers/parallel_state.py:593] > initializing pipeline model parallel with size 1\n",
      "[2025-06-02 13:35:56.010: I neuronx_distributed/parallel_layers/parallel_state.py:594] > initializing context model parallel with size 1\n",
      "[2025-06-02 13:35:56.010: I neuronx_distributed/parallel_layers/parallel_state.py:595] > initializing data parallel with size 1\n",
      "[2025-06-02 13:35:56.010: I neuronx_distributed/parallel_layers/parallel_state.py:596] > initializing world size to 8\n",
      "[2025-06-02 13:35:56.010: I neuronx_distributed/parallel_layers/parallel_state.py:343] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x70ec7a840ca0>, 'Ascending Ring PG Group')>\n",
      "[2025-06-02 13:35:56.011: I neuronx_distributed/parallel_layers/parallel_state.py:632] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1, 2, 3, 4, 5, 6, 7]]\n",
      "[2025-06-02 13:35:56.011: I neuronx_distributed/parallel_layers/parallel_state.py:633] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 13:35:56.011: I neuronx_distributed/parallel_layers/parallel_state.py:634] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 13:35:56.011: I neuronx_distributed/parallel_layers/parallel_state.py:635] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 13:35:56.011: I neuronx_distributed/parallel_layers/parallel_state.py:636] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 13:35:56.011: I neuronx_distributed/parallel_layers/parallel_state.py:637] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "INFO:Neuron:Generating 1 hlos for key: context_encoding_model\n",
      "INFO:Neuron:Started loading module context_encoding_model\n",
      "INFO:Neuron:Finished loading module context_encoding_model in 0.3605782985687256 seconds\n",
      "INFO:Neuron:generating HLO: context_encoding_model, input example shape = torch.Size([1, 16])\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:478: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:210: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=1, shape=torch.Size([1, 16]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:210: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:210: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=4, shape=torch.Size([1, 3]), dtype=torch.float32)\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:210: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:210: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "INFO:Neuron:Finished generating HLO for context_encoding_model in 8.811824083328247 seconds, input example shape = torch.Size([1, 16])\n",
      "INFO:Neuron:Generating 1 hlos for key: token_generation_model\n",
      "INFO:Neuron:Started loading module token_generation_model\n",
      "INFO:Neuron:Finished loading module token_generation_model in 0.13971686363220215 seconds\n",
      "INFO:Neuron:generating HLO: token_generation_model, input example shape = torch.Size([1, 1])\n",
      "INFO:Neuron:Finished generating HLO for token_generation_model in 9.776893615722656 seconds, input example shape = torch.Size([1, 1])\n",
      "INFO:Neuron:Generated all HLOs in 19.276326656341553 seconds\n",
      "INFO:Neuron:Starting compilation for the priority HLO\n",
      "INFO:Neuron:'token_generation_model' is the priority model with bucket rank 0\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:283: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n",
      "2025-06-02 13:36:15.000516:  7289  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --framework=XLA /tmp/nxd_model/token_generation_model/_tp0_bk0/model.MODULE_9b906898286ddf239aa0+91ef39e9.hlo_module.pb --output /tmp/nxd_model/token_generation_model/_tp0_bk0/model.MODULE_9b906898286ddf239aa0+91ef39e9.neff --target=trn1 --auto-cast=none --model-type=transformer --tensorizer-options=--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma  --lnc=1 --logfile=/tmp/nxd_model/token_generation_model/_tp0_bk0/log-neuron-cc.txt --enable-internal-neff-wrapper --verbose=35\n",
      ".........Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "INFO:Neuron:Done compilation for the priority HLO in 169.35613083839417 seconds\n",
      "INFO:Neuron:Updating the hlo module with optimized layout\n",
      "INFO:Neuron:Done optimizing weight layout for all HLOs in 0.3216278553009033 seconds\n",
      "INFO:Neuron:Starting compilation for all HLOs\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:245: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n",
      "2025-06-02 13:39:05.000174:  7289  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --framework=XLA /tmp/nxd_model/context_encoding_model/_tp0_bk0/model.MODULE_d4332219e6ee5f826cce+d43b5474.hlo_module.pb --output /tmp/nxd_model/context_encoding_model/_tp0_bk0/model.MODULE_d4332219e6ee5f826cce+d43b5474.neff --target=trn1 --auto-cast=none --model-type=transformer --tensorizer-options=--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma  --lnc=1 -O1 --internal-hlo2tensorizer-options= --modular-flow-mac-threshold=10  --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk0/log-neuron-cc.txt --verbose=35\n",
      ".Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "INFO:Neuron:Finished Compilation for all HLOs in 9.435595512390137 seconds\n",
      "......Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "INFO:Neuron:Done preparing weight layout transformation\n",
      "INFO:Neuron:Finished building model in 307.08067560195923 seconds\n",
      "INFO:Neuron:SKIPPING pre-sharding the checkpoints. The checkpoints will be sharded during load time.\n",
      "Compiling and tracing time: 307.11146965399985 seconds\n",
      "\n",
      "Loading model to Neuron...\n",
      "INFO:Neuron:Sharding weights on load...\n",
      "INFO:Neuron:Sharding Weights for ranks: 0...7\n",
      "[2025-06-02 13:41:03.157: I neuronx_distributed/parallel_layers/parallel_state.py:592] > initializing tensor model parallel with size 8\n",
      "[2025-06-02 13:41:03.157: I neuronx_distributed/parallel_layers/parallel_state.py:593] > initializing pipeline model parallel with size 1\n",
      "[2025-06-02 13:41:03.157: I neuronx_distributed/parallel_layers/parallel_state.py:594] > initializing context model parallel with size 1\n",
      "[2025-06-02 13:41:03.157: I neuronx_distributed/parallel_layers/parallel_state.py:595] > initializing data parallel with size 1\n",
      "[2025-06-02 13:41:03.158: I neuronx_distributed/parallel_layers/parallel_state.py:596] > initializing world size to 8\n",
      "[2025-06-02 13:41:03.158: I neuronx_distributed/parallel_layers/parallel_state.py:343] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x70ec7a840ca0>, 'Ascending Ring PG Group')>\n",
      "[2025-06-02 13:41:03.159: I neuronx_distributed/parallel_layers/parallel_state.py:632] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1, 2, 3, 4, 5, 6, 7]]\n",
      "[2025-06-02 13:41:03.159: I neuronx_distributed/parallel_layers/parallel_state.py:633] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 13:41:03.159: I neuronx_distributed/parallel_layers/parallel_state.py:634] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 13:41:03.159: I neuronx_distributed/parallel_layers/parallel_state.py:635] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 13:41:03.159: I neuronx_distributed/parallel_layers/parallel_state.py:636] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 13:41:03.159: I neuronx_distributed/parallel_layers/parallel_state.py:637] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "INFO:Neuron:Done Sharding weights in 3.519328597999902\n",
      "INFO:Neuron:Finished weights loading in 16.628388952000023 seconds\n",
      "INFO:Neuron:Warming up the model.\n",
      "2025-Jun-02 13:41:22.0009 7289:8468 [7] nccl_net_ofi_create_plugin:211 CCOM WARN NET/OFI Failed to initialize sendrecv protocol\n",
      "2025-Jun-02 13:41:22.0010 7289:8468 [7] nccl_net_ofi_create_plugin:334 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2025-Jun-02 13:41:22.0011 7289:8468 [7] nccl_net_ofi_init:155 CCOM WARN NET/OFI Initializing plugin failed\n",
      "2025-Jun-02 13:41:22.0012 7289:8468 [7] net_plugin.cc:94 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n",
      "INFO:Neuron:Warmup completed in 0.33977651596069336 seconds.\n",
      "Total model loading time: 19.222302051000042 seconds\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "\n",
      "Checking accuracy by logit matching\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/utils/accuracy.py:363: UserWarning: input_len + num_tokens_to_check exceeds max_context_length. If output divergences at an index greater than max_context_length, a ValueError will occur because the next input len exceeds max_context_length. To avoid this, set num_tokens_to_check to a value of max_context_length - input_len or less.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:08<00:00,  1.58it/s]\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n",
      "Expected Output:  [\", that is the question. Whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune\"] tensor([[   11,   429,   374,   279,  3405,    13, 13139,   364,    83,   285,\n",
      "         13049,  1536,   304,   279,  3971,   311,  7676,   279,  1739,   819,\n",
      "           323, 36957,   315, 54488, 32315]])\n",
      "Expected Logits Shape:  torch.Size([25, 1, 152064])\n",
      "Actual Output:  [\", that is the question. Whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune\"] tensor([[   11,   429,   374,   279,  3405,    13, 13139,   364,    83,   285,\n",
      "         13049,  1536,   304,   279,  3971,   311,  7676,   279,  1739,   819,\n",
      "           323, 36957,   315, 54488, 32315]])\n",
      "Actual Logits Shape:  torch.Size([25, 1, 152064])\n",
      "Passed logits validation!\n",
      "\n",
      "Generating outputs...\n",
      "Prompts: ['To be, or not to be']\n",
      "Generated outputs:\n",
      "Output 0: To be, or not to be, that is the question. Whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune\n",
      "Starting end-to-end benchmark with 20\n",
      "Benchmark completed and its result is as following\n",
      "{\n",
      "    \"e2e_model\": {\n",
      "        \"latency_ms_p50\": 569.0377950668335,\n",
      "        \"latency_ms_p90\": 570.0641632080078,\n",
      "        \"latency_ms_p95\": 570.2431917190552,\n",
      "        \"latency_ms_p99\": 570.8965921401978,\n",
      "        \"latency_ms_p100\": 571.0599422454834,\n",
      "        \"latency_ms_avg\": 569.459593296051,\n",
      "        \"throughput\": 56.19362703995017\n",
      "    },\n",
      "    \"context_encoding_model\": {\n",
      "        \"latency_ms_p50\": 41.747450828552246,\n",
      "        \"latency_ms_p90\": 42.02606678009033,\n",
      "        \"latency_ms_p95\": 42.056477069854736,\n",
      "        \"latency_ms_p99\": 42.05883264541626,\n",
      "        \"latency_ms_p100\": 42.05942153930664,\n",
      "        \"latency_ms_avg\": 41.80266857147217,\n",
      "        \"throughput\": 382.75068426897144\n",
      "    },\n",
      "    \"token_generation_model\": {\n",
      "        \"latency_ms_p50\": 33.631086349487305,\n",
      "        \"latency_ms_p90\": 33.74745845794678,\n",
      "        \"latency_ms_p95\": 33.88720750808716,\n",
      "        \"latency_ms_p99\": 34.08886194229126,\n",
      "        \"latency_ms_p100\": 34.223079681396484,\n",
      "        \"latency_ms_avg\": 33.66035064061483,\n",
      "        \"throughput\": 31.68911334451813\n",
      "    }\n",
      "}\n",
      "Completed saving result to benchmark_report.json\n"
     ]
    }
   ],
   "source": [
    "!inference_demo \\\n",
    "    --model-type qwen2 \\\n",
    "    --task-type causal-lm \\\n",
    "    run \\\n",
    "    --model-path /home/ubuntu/model_hf_qwen/qwen2 \\\n",
    "    --compiled-model-path /home/ubuntu/traced_model_qwen/qwen2/logit \\\n",
    "    --torch-dtype bfloat16 \\\n",
    "    --tp-degree 8 \\\n",
    "    --batch-size 1 \\\n",
    "    --max-context-length 16 \\\n",
    "    --seq-len 32 \\\n",
    "    --top-k 1 \\\n",
    "    --pad-token-id 151645 \\\n",
    "    --prompt \"To be, or not to be\" \\\n",
    "    --check-accuracy-mode logit-matching \\\n",
    "    --benchmark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_2_6_nxd_inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
